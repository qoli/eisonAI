# eisonAI 長文閱讀 Pipeline 規格（Spec）

## 1. 目的

本 pipeline 旨在**手機端**、僅使用**小型本地語言模型（qwen3-0.6B）**的前提下，對超長文本提供**穩定、低幻覺、可預期**的閱讀輔助體驗。

核心目標：
- 支援超出模型 context window 的長文處理
- 避免單次摘要造成語義坍縮與幻覺放大
- 將模型能力限制在「可控、可預測」範圍內
- 明確區分**閱讀理解價值**與**展示用摘要**

## 2. 設計原則

### 2.1 端側優先（On-Device First）
- 全流程在裝置端完成
- 不依賴雲端模型或更大參數模型
- 適用於資源受限環境（手機）

### 2.2 Less-is-More（小模型友善）
- Prompt 避免高階抽象語義
- 避免多重任務、隱含目標與過度約束
- 將「理解責任」留在系統層，而非模型層

### 2.3 職責分離（Separation of Concerns）
- **閱讀錨點（Reading Anchors）**與**摘要展示（Presentation Summary）**明確分離
- 精度優先的輸出與展示導向的輸出分層處理

## 3. Pipeline 總覽

```
原文
  ↓
Step 1  Token 切割
  ↓
Step 2  Chunk 級閱讀錨點抽取（array X）
  ↓
Step 3  展示用摘要生成
  ↓
輸出：array X + summary
```

## 4. Pipeline 詳細步驟

### Step 1：長文切割（Chunking）

**目的**  
將超長文本切割為可被模型安全處理的段落。

**實作方式**
- 使用 `NLTokenizer`
- 以 **2000 token** 為單位切割

**設計理由**
- 保留段落語義完整性
- 避免接近 4k context 上限造成不穩定行為
- 為後續 chunk 級處理提供穩定邊界

**輸出**
- `chunks[]`：原文段落陣列

### Step 2：Chunk 級閱讀錨點抽取（核心步驟）

**角色定位**
- 模型角色：文字整理員
- 任務性質：閱讀輔助（非摘要生成）

**Prompt（workingVersion）**

```text
你是一個文字整理員。

你目前的任務是，正在協助用戶完整閱讀超長內容。

- 當前這是原文中的一個段落（chunks 1 of 4）
- 擷取此文章的關鍵點
```

**設計重點**
- 明確角色，限制模型行為空間
- 明確「這不是全文」，防止過早總結
- 任務描述保持輕量，避免高階語義判斷

**行為預期**
- 僅基於當前段落
- 整理、重排、提取原文中已有資訊
- 避免創作與外推

**輸出**
- `array X[]`：逐段關鍵點結果  
  （此為整個 pipeline 的主要價值輸出）

### Step 3：展示用摘要生成（非精度導向）

**目的**
- 提供「快速掃描」用的總覽內容
- 不承擔嚴格正確性或忠於原文的責任

**輸入**
- `array X[]`（Step 2 的中間產物）

**Prompt（newPrompt）**

```text
將內容整理為簡短簡報，包含重點摘要。

輸出要求：

- 合適的格式結構
- 使用繁體中文。
```

**設計說明**
- 不再使用角色定位
- 不再要求閱讀語境或忠於原文
- 明確將此步驟定位為 Presentation Layer

**設計取捨**
- 接受模型使用其最穩定的摘要模板
- 接受可能的語義簡化與概括
- 透過「職責降級」避免幻覺放大影響核心價值

**輸出**
- `summary`：簡短、可閱讀的摘要文本

## 5. 最終輸出

系統同時交付：
- `array X`：逐段閱讀錨點  
  - 高可信度  
  - 用於實際理解與閱讀輔助
- `summary`：展示用摘要  
  - 低風險、低成本  
  - 用於快速掃描與 UI 呈現

## 6. 已知限制

- Step 3 的摘要不保證完全忠於原文
- 不嘗試在端側進行全文級深度推理
- 不解決跨段落高階語義整合問題（屬於大模型責任）

## 7. 設計結論

本 pipeline 並非追求「一次生成完美摘要」，而是透過**分段閱讀錨點 + 展示用摘要**的結構，在小模型與端側限制下提供**穩定、可預期、可長期使用**的閱讀系統。
